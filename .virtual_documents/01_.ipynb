import os
from dotenv import load_dotenv
load_dotenv() 
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_API_KEY"] = os.getenv("LANGSMITH_API_KEY")


from langchain_core.messages import SystemMessage
from langgraph.graph import StateGraph
from langgraph.graph import StateGraph, START, END


from typing import TypedDict, NotRequired, Required, Dict, List, Any

#Define the input state
class InputState(TypedDict, total=False):
    company: Required[str]
    company_url: NotRequired[str]
    hq_location: NotRequired[str]
    industry: NotRequired[str]

class ResearchState(InputState):
    site_scrape: Dict[str, Any]
    messages: List[Any]
    financial_data: Dict[str, Any]
    news_data: Dict[str, Any]
    industry_data: Dict[str, Any]
    company_data: Dict[str, Any]
    curated_financial_data: Dict[str, Any]
    curated_news_data: Dict[str, Any]
    curated_industry_data: Dict[str, Any]
    curated_company_data: Dict[str, Any]
    financial_briefing: str
    news_briefing: str
    industry_briefing: str
    company_briefing: str
    references: List[str]
    briefings: Dict[str, Any]
    report: str





import os
from tavily import TavilyClient
from langchain_core.messages import AIMessage

# Initialize Tavily client
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def grounding_node(state):
    company = state.get("company", "Unknown Company")
    url = state.get("company_url")

    message = f"ðŸŽ¯ Initiating research for {company}."

    site_scrape = {}

    if url:
        message += f"\nðŸŒ Analyzing company website: {url}"

        try:
            extraction = tavily_client.extract(url, extract_depth="basic")
            raw_contents = []

            for item in extraction.get("results", []):
                content = item.get("raw_content")
                if content:
                    raw_contents.append(content)

            if raw_contents:
                site_scrape = {
                    "title": company,
                    "raw_content": "\n\n".join(raw_contents)
                }
                message += "\nâœ… Successfully extracted content from website."
            else:
                message += "\nâš ï¸ No content found on website."
        except Exception as e:
            message += f"\nâš ï¸ Error extracting content: {str(e)}"
            # You can choose to log or raise the error depending on your system
    else:
        message += "\nâ© No company URL provided, skipping website analysis."

    # Add this message to state
    messages = state.get("messages", [])
    messages.append(AIMessage(content=message))
    state["messages"] = messages
    state["site_scrape"] = site_scrape

    return {
        "message": message,
        "site_scrape": site_scrape
    }



import os
from openai import OpenAI
from tavily import TavilyClient
from langchain_core.messages import AIMessage

# Initialize OpenAI and Tavily clients
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def financial_analyst_node(state):
    company = state.get("company", "Unknown Company")
    industry = state.get("industry", "Unknown Industry")
    hq = state.get("hq_location", "Unknown HQ")

    # Generate financial queries using OpenAI
    prompt = f"""
Generate queries on the financial analysis of {company} in the {industry} industry such as:
- Fundraising history and valuation
- Financial statements and key metrics
- Revenue and profit sources

Important Guidelines:
- Focus ONLY on {company}-specific information
- Make queries very brief and to the point
- Provide exactly 4 search queries (one per line), with no hyphens or dashes
- DO NOT make assumptions about the industry - use only the provided industry information
"""

    response = openai_client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[
            {"role": "system", "content": f"You are researching {company} in the {industry} industry."},
            {"role": "user", "content": prompt}
        ],
        temperature=0,
        max_tokens=512,
    )

    queries_text = response.choices[0].message.content.strip()
    queries = [line.strip() for line in queries_text.split("\n") if line.strip()]

    # Update message in state
    messages = state.get("messages", [])
    messages.append(AIMessage(content="ðŸ” Subqueries for financial analysis:\n" + "\n".join([f"â€¢ {q}" for q in queries])))
    state["messages"] = messages

    # Search documents using Tavily
    financial_data = {}
    for query in queries:
        results = tavily_client.search(query, search_depth="basic", max_results=5)
        for result in results.get("results", []):
            url = result.get("url")
            if not url or not result.get("content"):
                continue
            financial_data[url] = {
                "title": result.get("title", ""),
                "content": result.get("content", ""),
                "query": query,
                "url": url,
                "source": "web_search",
                "score": result.get("score", 0.0)
            }

    # Final message
    completion_msg = f"Completed analysis with {len(financial_data)} documents."
    messages.append(AIMessage(content=completion_msg))
    state["messages"] = messages
    state["financial_data"] = financial_data

    return {
        "message": completion_msg,
        "financial_data": financial_data,
        "analyst_type": "financial_analyzer",
        "queries": queries
    }



from langchain_core.messages import AIMessage

def collector_node(state):
    company = state.get("company", "Unknown Company")

    # Initial message
    msg_lines = [f"ðŸ“¦ Collecting research data for {company}:"]

    # Check presence of each type of data
    research_types = {
        "financial_data": "ðŸ’° Financial",
        # "news_data": "ðŸ“° News",
        # "industry_data": "ðŸ­ Industry",
        # "company_data": "ðŸ¢ Company"
    }

    for field, label in research_types.items():
        data = state.get(field, {})
        if data:
            msg_lines.append(f"â€¢ {label}: {len(data)} documents collected")
        else:
            msg_lines.append(f"â€¢ {label}: No data found")

    # Update messages in state
    messages = state.get("messages", [])
    messages.append(AIMessage(content="\n".join(msg_lines)))
    state["messages"] = messages

    return state



import logging
from urllib.parse import urlparse, urljoin
from langchain_core.messages import AIMessage

logger = logging.getLogger(__name__)

def curator_node(state):
    relevance_threshold = 0.4
    company = state.get("company", "Unknown Company")
    industry = state.get("industry", "Unknown Industry")
    context = {
        "company": company,
        "industry": industry,
        "hq_location": state.get("hq_location", "Unknown")
    }

    msg = [f"ðŸ” Curating research data for {company}"]
    data_types = {
        'financial_data': ('ðŸ’° Financial', 'financial'),
        # 'news_data': ('ðŸ“° News', 'news'),
        # 'industry_data': ('ðŸ­ Industry', 'industry'),
        # 'company_data': ('ðŸ¢ Company', 'company')
    }

    doc_counts = {}

    for data_field, (emoji, doc_type) in data_types.items():
        raw_docs = state.get(data_field, {})
        if not raw_docs:
            continue

        # Normalize URLs
        unique_docs = {}
        for url, doc in raw_docs.items():
            try:
                parsed = urlparse(url)
                if not parsed.scheme:
                    url = urljoin('https://', url)
                clean_url = parsed._replace(query='', fragment='').geturl()
                doc['url'] = clean_url
                doc['doc_type'] = doc_type
                unique_docs[clean_url] = doc
            except Exception:
                continue

        docs = list(unique_docs.values())
        msg.append(f"\n{emoji}: Found {len(docs)} documents")

        # Evaluate relevance
        evaluated_docs = []
        for doc in docs:
            try:
                score = float(doc.get("score", 0))
                if score >= relevance_threshold:
                    doc["evaluation"] = {"overall_score": score, "query": doc.get("query", "")}
                    evaluated_docs.append(doc)

            except Exception as e:
                logger.warning(f"Skipping document due to score error: {e}")

        evaluated_docs.sort(key=lambda x: x["evaluation"]["overall_score"], reverse=True)
        top_docs = evaluated_docs[:30]

        doc_counts[data_field] = {
            "initial": len(docs),
            "kept": len(top_docs)
        }

        if top_docs:
            msg.append(f"  âœ“ Kept {len(top_docs)} relevant documents")
            state[f"curated_{data_field}"] = {doc["url"]: doc for doc in top_docs}
        else:
            msg.append("  âš ï¸ No documents met relevance threshold")

    # Reference generation
    ref_urls, ref_titles, ref_info = process_references_from_search_results(state)
    state["references"] = ref_urls
    state["reference_titles"] = ref_titles
    state["reference_info"] = ref_info

    # Final summary
    messages = state.get("messages", [])
    messages.append(AIMessage(content="\n".join(msg)))
    state["messages"] = messages
    return state



import os
from tavily import TavilyClient
from langchain_core.messages import AIMessage

# Initialize Tavily sync client
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

def enricher_node(state):
    company = state.get("company", "Unknown Company")
    msg = [f"ðŸ“š Enriching curated data for {company}:"]

    data_types = {
        "financial_data": ("ðŸ’° Financial", "financial"),
        "news_data": ("ðŸ“° News", "news"),
        "industry_data": ("ðŸ­ Industry", "industry"),
        "company_data": ("ðŸ¢ Company", "company")
    }

    for data_field, (label, category) in data_types.items():
        curated_field = f"curated_{data_field}"
        curated_docs = state.get(curated_field, {})
        if not curated_docs:
            msg.append(f"\nâ€¢ No curated {label} documents to enrich")
            continue

        docs_to_enrich = {
            url: doc for url, doc in curated_docs.items() if not doc.get("raw_content")
        }

        if not docs_to_enrich:
            msg.append(f"\nâ€¢ All {label} documents already have raw content")
            continue

        msg.append(f"\nâ€¢ Enriching {len(docs_to_enrich)} {label} documents...")

        enriched_count = 0
        for url in docs_to_enrich:
            try:
                result = tavily_client.extract(url)
                content = result["results"][0].get("raw_content", "") if result.get("results") else ""
                if content:
                    curated_docs[url]["raw_content"] = content
                    enriched_count += 1
            except Exception as e:
                print(f"Failed to enrich {url}: {e}")

        state[curated_field] = curated_docs
        msg.append(f"  âœ“ Enriched {enriched_count}/{len(docs_to_enrich)} documents.")

    # Final message update
    messages = state.get("messages", [])
    messages.append(AIMessage(content="\n".join(msg)))
    state["messages"] = messages

    return state



import os
import logging
# import google.generativeai as genai
from langchain_core.messages import AIMessage

# Setup logging
logger = logging.getLogger(__name__)

# Gemini setup
# genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
from langchain.chat_models import init_chat_model
gemini_model = init_chat_model("gpt-4o-mini", model_provider="openai")
# gemini_model = genai.GenerativeModel("gemini-2.0")

MAX_DOC_LENGTH = 8000

def generate_briefing_text(docs, category, context):
    company = context.get("company", "Unknown")
    industry = context.get("industry", "Unknown")
    hq_location = context.get("hq_location", "Unknown")

    prompts = {
#         'company': f"""Create a focused company briefing for {company}, a {industry} company based in {hq_location}...

# [shortened here for clarity â€” paste full prompt as in original]""",
#         'industry': f"""Create a focused industry briefing for {company}, a {industry} company based in {hq_location}...

# [shortened here for clarity â€” paste full prompt as in original]""",
        'financial': f"""Create a focused financial briefing for {company}, a {industry} company based in {hq_location}...

[shortened here for clarity â€” paste full prompt as in original]""",
#         'news': f"""Create a focused news briefing for {company}, a {industry} company based in {hq_location}...

# [shortened here for clarity â€” paste full prompt as in original]""",
    }

    # Flatten docs
    items = list(docs.items()) if isinstance(docs, dict) else [
        (doc.get("url", f"doc_{i}"), doc) for i, doc in enumerate(docs)
    ]
    sorted_docs = sorted(items, key=lambda x: float(x[1].get("evaluation", {}).get("overall_score", 0)), reverse=True)

    total_length = 0
    doc_texts = []
    for _, doc in sorted_docs:
        title = doc.get("title", "")
        content = doc.get("raw_content") or doc.get("content", "")
        content = content[:MAX_DOC_LENGTH] + "... [truncated]" if len(content) > MAX_DOC_LENGTH else content
        doc_entry = f"Title: {title}\n\nContent: {content}"
        if total_length + len(doc_entry) < 120000:
            doc_texts.append(doc_entry)
            total_length += len(doc_entry)
        else:
            break

    separator = "\n" + "-" * 40 + "\n"
    prompt = f"""{prompts.get(category, 'Create a focused briefing for the company.')}

{separator}{separator.join(doc_texts)}{separator}
"""

    try:
        logger.info(f"Calling Gemini for {category} briefing...")
        response = gemini_model.generate_content(prompt)
        return response.text.strip() if response.text else ""
    except Exception as e:
        logger.error(f"Gemini error for {category}: {e}")
        return ""

def briefing_node(state):
    company = state.get("company", "Unknown Company")
    industry = state.get("industry", "Unknown")
    hq_location = state.get("hq_location", "Unknown")

    context = {
        "company": company,
        "industry": industry,
        "hq_location": hq_location
    }

    categories = {
        "financial_data": ("financial", "financial_briefing"),
        # "news_data": ("news", "news_briefing"),
        # "industry_data": ("industry", "industry_briefing"),
        # "company_data": ("company", "company_briefing")
    }

    briefings = {}
    msg = [f"ðŸ“ Generated briefings for {company}:"]

    for data_field, (category, briefing_key) in categories.items():
        curated_key = f"curated_{data_field}"
        curated_data = state.get(curated_key, {})
        if curated_data:
            text = generate_briefing_text(curated_data, category, context)
            if text:
                state[briefing_key] = text
                briefings[category] = text
                msg.append(f"â€¢ {category.title()} briefing created ({len(text)} characters)")
            else:
                state[briefing_key] = ""
                msg.append(f"â€¢ {category.title()} briefing failed")
        else:
            state[briefing_key] = ""
            msg.append(f"â€¢ No data for {category.title()}")

    state["briefings"] = briefings
    messages = state.get("messages", [])
    messages.append(AIMessage(content="\n".join(msg)))
    state["messages"] = messages

    return state



workflow = StateGraph(ResearchState)

workflow.add_node("grounding", grounding_node)
workflow.add_node("financial_analyst", financial_analyst_node)
workflow.add_node("collector_node", collector_node)
workflow.add_node("enricher_node", enricher_node)
workflow.add_node("briefing_node", briefing_node)

# Start the edges
workflow.add_edge(START, "grounding")
# Configure workflow edges
workflow.add_edge("grounding", "financial_analyst")
workflow.add_edge('financial_analyst', 'collector_node')
workflow.add_edge('collector_node', 'enricher_node')
workflow.add_edge('enricher_node', 'briefing_node')
workflow.add_edge('briefing_node', END)
graph = workflow.compile()


graph


input_state = {
    "company": 'Turing',
    "company_url": 'https://www.turing.com/',
    "hq_location": 'US',
    "industry": 'AI'
}


from pprint import pprint


for step in graph.stream(
    input_state,
    stream_mode="values",
):
    print("--------------------------- \n\n")
    print(step)



